# FleetPulse Environment Configuration
# Copy this file to .env and customize the values for your environment

# Data directory path for persistent storage
# Default: ./data (relative to docker-compose.yml location)
FLEETPULSE_DATA_PATH=./data

# Alternative examples:
# FLEETPULSE_DATA_PATH=/mnt/data/dockervolumes/fleetpulse
# FLEETPULSE_DATA_PATH=/home/user/fleetpulse-data

# Allowed CORS origins (comma-separated)
# Default: * (allow all origins)
ALLOWED_ORIGINS=*

# Example for production:
# ALLOWED_ORIGINS=https://yourdomain.com,https://www.yourdomain.com

# Server Configuration
# Deployment mode: "uvicorn" (simple) or "gunicorn" (production)
# Default: uvicorn (recommended for single-container deployments)
# Use "gunicorn" for high-traffic production deployments
DEPLOYMENT_MODE=uvicorn

# When to use Uvicorn mode (DEPLOYMENT_MODE=uvicorn):
# - Single-container deployments (default recommended)
# - Development environments
# - Low to medium traffic (< 100 concurrent users)
# - Simplified setup and debugging
# - Lower memory footprint
# - Faster startup times

# When to use Gunicorn mode (DEPLOYMENT_MODE=gunicorn):
# - High-traffic production deployments (> 100 concurrent users)
# - Need for process-level fault tolerance
# - CPU-intensive workloads
# - Multi-core utilization requirements

# Number of Gunicorn workers (only used when DEPLOYMENT_MODE=gunicorn)
# Default: 2 (optimal for single-container deployment)
# For high-traffic deployments, consider increasing to 4-8
GUNICORN_WORKERS=2

# Database Configuration
# Force database recreation on startup (drops and recreates all tables)
# Default: false (recommended for production)
# Set to true for development/testing when you need to reset the database
FORCE_DB_RECREATE=false

# OpenTelemetry Configuration (Backend Only)
# Frontend telemetry has been disabled
# Enable or disable backend telemetry collection
# Default: true (enable observability)
OTEL_ENABLE_TELEMETRY=true

# Deployment environment (development, staging, production)
# Default: development
OTEL_ENVIRONMENT=development

# Telemetry exporter type (jaeger, otlp, console)
# Default: jaeger (recommended for development and testing)
# Use "otlp" for production with OpenTelemetry Collector
# Use "console" for debugging (logs traces to console)
OTEL_EXPORTER_TYPE=jaeger

# Trace sampling rate (0.0 to 1.0)
# Default: 1.0 (sample all traces in development)
# For production, consider 0.1 (10%) or lower to reduce overhead
OTEL_TRACE_SAMPLE_RATE=1.0

# Jaeger Configuration (when OTEL_EXPORTER_TYPE=jaeger)
# Jaeger endpoints are automatically configured in docker-compose
# External Jaeger instance example:
# OTEL_EXPORTER_JAEGER_ENDPOINT=http://your-jaeger-instance:14268/api/traces

# OTLP Configuration (when OTEL_EXPORTER_TYPE=otlp)
# Default OTLP Collector endpoint (configured in docker-compose.yml)
# OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
# External OTLP Collector example:
# OTEL_EXPORTER_OTLP_ENDPOINT=http://your-otel-collector:4317

# Production Telemetry Example:
# OTEL_ENABLE_TELEMETRY=true
# OTEL_ENVIRONMENT=production
# OTEL_EXPORTER_TYPE=otlp
# OTEL_TRACE_SAMPLE_RATE=0.1
# OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317

# MCP Server - Model endpoint for proxy functionality
# The MCP server /mcp/v1/proxy endpoint can forward requests to external LLM APIs
# Set this to your preferred LLM API endpoint

# Local Ollama (included in docker-compose - recommended default)
# For development, use lightweight models like phi3:mini or tinyllama:1.1b
MODEL_ENDPOINT_URL=http://ollama:11434/api/chat

# OpenAI API
# MODEL_ENDPOINT_URL=https://api.openai.com/v1/chat/completions
# OPENAI_API_KEY=sk-your-openai-api-key-here

# Local Ollama on host machine (if running outside Docker)
# MODEL_ENDPOINT_URL=http://host.docker.internal:11434/api/chat

# Local LM Studio  
# MODEL_ENDPOINT_URL=http://host.docker.internal:1234/v1/chat/completions

# Azure OpenAI
# MODEL_ENDPOINT_URL=https://your-resource.openai.azure.com/openai/deployments/your-model/chat/completions
# AZURE_OPENAI_API_KEY=your-azure-api-key

# Anthropic Claude API
# MODEL_ENDPOINT_URL=https://api.anthropic.com/v1/messages
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key

# MCP Server Configuration
LOG_LEVEL=info
LOG_FORMAT=json
CORS_ORIGIN=*
CORS_CREDENTIALS=false
HELMET_ENABLED=true
REQUEST_TIMEOUT=30000
MAX_REQUEST_SIZE=10mb
