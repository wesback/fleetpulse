---
# Sample Docker Compose file using pre-built containers from wesback/fleetpulse
# This file demonstrates how to deploy FleetPulse using the published Docker images

services:
  backend:
    image: wesback/fleetpulse-backend:latest
    container_name: fleetpulse-backend
    ports:
      - "8000:8000"  # Backend API
    volumes:
      # Mount data directory for persistent storage
      - fleetpulse_data:/data
      # Optional: Mount configuration files
      # - ./config:/app/config:ro
    environment:
      # Backend configuration
      - FLEETPULSE_DATA_DIR=/data
      - DEPLOYMENT_MODE=${DEPLOYMENT_MODE:-uvicorn}
      - GUNICORN_WORKERS=${GUNICORN_WORKERS:-2}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - fleetpulse_network

  frontend:
    image: wesback/fleetpulse-frontend:latest
    container_name: fleetpulse-frontend
    ports:
      - "8080:80"  # Frontend UI
    depends_on:
      - backend
    restart: unless-stopped
    networks:
      - fleetpulse_network

  # FleetPulse MCP Server - TypeScript-based Model Context Protocol server
  # Provides intelligent natural language query processing for FleetPulse data
  mcp:
    image: wesback/fleetpulse-mcp:latest
    container_name: fleetpulse-mcp
    ports:
      - "8001:8001"  # MCP Server API
    environment:
      # FleetPulse backend connection - REQUIRED
      - FLEETPULSE_API_URL=http://backend:8000
      
      # MCP server configuration
      - MCP_PORT=8001
      - MCP_HOST=0.0.0.0
      
      # Model endpoint for proxy functionality (optional)
      # This is used by the /mcp/v1/proxy endpoint to forward requests to LLM APIs
      # 
      # Using the included Ollama service (recommended for self-hosted):
      - MODEL_ENDPOINT_URL=${MODEL_ENDPOINT_URL:-http://ollama:11434/api/chat}
      #
      # Alternative external services:
      # - OpenAI API: https://api.openai.com/v1/chat/completions
      # - Local Ollama on host: http://host.docker.internal:11434/api/chat
      # - Local LM Studio: http://host.docker.internal:1234/v1/chat/completions
      # - Azure OpenAI: https://your-resource.openai.azure.com/openai/deployments/your-model/chat/completions
      
      # API Keys for external model services (set in .env file)
      # - OPENAI_API_KEY=${OPENAI_API_KEY}
      # - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY}
      
      # Logging configuration
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - LOG_FORMAT=${LOG_FORMAT:-json}
      
      # CORS and security settings
      - CORS_ORIGIN=${CORS_ORIGIN:-*}
      - CORS_CREDENTIALS=${CORS_CREDENTIALS:-false}
      - HELMET_ENABLED=${HELMET_ENABLED:-true}
      
      # Request handling
      - REQUEST_TIMEOUT=${REQUEST_TIMEOUT:-30000}
      - MAX_REQUEST_SIZE=${MAX_REQUEST_SIZE:-10mb}
    depends_on:
      - backend
      - ollama  # Add dependency on Ollama service
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - fleetpulse_network

  # Optional: Add Ollama for local LLM inference (recommended for self-hosted deployments)
  ollama:
    image: ollama/ollama:latest
    container_name: fleetpulse-ollama
    ports:
      - "11434:11434"  # Ollama API
    volumes:
      - ollama_data:/root/.ollama  # Persistent model storage
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
    # Uncomment for GPU support (requires nvidia-docker)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    networks:
      - fleetpulse_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Optional: Add a database service if your application requires one
  # database:
  #   image: postgres:15-alpine
  #   container_name: fleetpulse-db
  #   environment:
  #     POSTGRES_DB: fleetpulse
  #     POSTGRES_USER: fleetpulse
  #     POSTGRES_PASSWORD: ${DB_PASSWORD:-changeme}
  #   volumes:
  #     - postgres_data:/var/lib/postgresql/data
  #   ports:
  #     - "5432:5432"
  #   restart: unless-stopped

  # Optional: Add Redis for caching/session storage
  # redis:
  #   image: redis:7-alpine
  #   container_name: fleetpulse-redis
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - redis_data:/data
  #   restart: unless-stopped

volumes:
  fleetpulse_data:
    driver: local
  ollama_data:
    driver: local
  # postgres_data:
  #   driver: local
  # redis_data:
  #   driver: local

# Create a custom network
networks:
  fleetpulse_network:
    driver: bridge

# Usage Instructions:
# 1. Copy this file to your deployment server
# 2. Rename it to docker-compose.yml or use: docker-compose -f docker-compose.sample.yml up
# 3. Create a .env file for sensitive configuration values like API keys:
#    MODEL_ENDPOINT_URL=https://api.openai.com/v1/chat/completions
#    OPENAI_API_KEY=sk-your-openai-api-key-here
# 4. Update environment variables and port mappings as needed
# 5. Run: docker-compose up -d
# 6. Download models for Ollama (after containers are running):
#    # Lightweight development models:
#    docker exec -it fleetpulse-ollama ollama pull phi3:mini        # 2.3GB - Recommended for dev
#    docker exec -it fleetpulse-ollama ollama pull tinyllama:1.1b   # 0.6GB - Ultra fast
#    # Production models:
#    docker exec -it fleetpulse-ollama ollama pull llama3.1:8b      # 4.7GB - Full featured
#    docker exec -it fleetpulse-ollama ollama pull mistral:7b       # 4.1GB - Good performance
# 7. Access the services:
#    - Frontend UI: http://localhost:8080
#    - Backend API: http://localhost:8000
#    - MCP Server: http://localhost:8001
#      * Health: http://localhost:8001/health
#      * OpenAPI Spec: http://localhost:8001/mcp/v1/openapi
#      * Context Processing: POST http://localhost:8001/mcp/v1/context
#      * FleetPulse Queries: POST http://localhost:8001/mcp/v1/query
#      * Model Proxy: POST http://localhost:8001/mcp/v1/proxy
#    - Ollama API: http://localhost:11434
#      * Available models: GET http://localhost:11434/api/tags
#      * Chat endpoint: POST http://localhost:11434/api/chat
#
# MCP Server Features:
# - Natural language queries about FleetPulse data (via /query endpoint)
# - Model Context Protocol implementation
# - OpenAPI 3.1 specification
# - Request proxying to model endpoints (via /proxy endpoint)
# - TypeScript-based with full type safety
#
# Model Proxy Functionality:
# The /mcp/v1/proxy endpoint allows you to:
# - Forward requests to external LLM APIs (OpenAI, Anthropic, etc.)
# - Use the included Ollama service for local AI inference
# - Transform requests/responses through the MCP server
# - Add authentication headers and monitoring
# - Centralize all AI model interactions
# Set MODEL_ENDPOINT_URL to your preferred LLM API endpoint
#
# Ollama Usage:
# - The included Ollama service provides local LLM inference
# - No external API keys required for basic functionality
# - Models are downloaded and cached in the ollama_data volume
# - Supports GPU acceleration with proper Docker configuration
# - For production, consider downloading models during build time
